setwd("~/")
knitr::opts_chunk$set(echo = TRUE)
# with cmd/ctrl-enter, run single code lines within a chunk
if (!require("easyPubMed")) install.packages("easyPubMed")
# after installation, load the package
library(easyPubMed)
# replace the blank ______ with your search terms
query <- '((((microbiome)) AND (core)) AND (("2016"[Date - Publication] : "2020"[Date - Publication])))'
entrez_id <- get_pubmed_ids(query)
# How many results did your query return?
entrez_id$microbiome
# How many of these results will you use to ID keywords?
abstracts_txt <- batch_pubmed_download(pubmed_query_string = query,
format = "medline",
batch_size = ___)
View(entrez_id)
# replace the blank ______ with your search terms
query <- '((((microbiome)) AND (core)) AND (("2016"[Date - Publication] : "2020"[Date - Publication])))'
entrez_id <- get_pubmed_ids(query)
# How many results did your query return?
entrez_id$microbiome
# How many of these results will you use to ID keywords?
abstracts_txt <- batch_pubmed_download(pubmed_query_string = query,
format = "medline",
batch_size = 2113)
abstracts_txt <- batch_pubmed_download(pubmed_query_string = query, format = "medline",batch_size = 80)
``Error in batch_pubmed_download(pubmed_query_string = query, format = "medline",  :
# replace the blank ______ with your search terms
query <- '((((microbiome)) AND (core) AND (function)) AND (("2016"[Date - Publication] : "2020"[Date - Publication])))'
entrez_id <- get_pubmed_ids(query)
# How many results did your query return?
entrez_id$microbiome
# How many of these results will you use to ID keywords?
abstracts_txt <- batch_pubmed_download(pubmed_query_string = query,
format = "medline",
batch_size = 2113)
abstracts_txt <- batch_pubmed_download(pubmed_query_string = query, format = "medline",batch_size = 80)
if (!require("remotes")) install.packages("remotes")
if (!require("litsearchr"))
library(litsearchr)
library(litsearchr)
# We need to import the PubMed results as a data frame
PMimport <- import_results(file = "easyPubMed_data_001.txt", verbose = TRUE)
# Article titles and abstracts often contain key terms.
# Can you identify which columns that information is found in?
rakedkeywords <- extract_terms(text = paste(PMimport$title,
PMimport$abstract),
method = "fakerake", language = "English",
# How many times should each key term occur (Freq)?
# What is the minimum number of words that need to
# be in each key term (n)?
min_freq = 2, ngrams = TRUE, min_n = 1)
# Did you notice there are some keywords already identified within
# PMimport? These are known as "tagged" keywords. Can you replicate
# the above extract_terms call for the tagged keywords in PMimport?
taggedkeywords <- extract_terms(keywords = PMimport$keywords, method = "tagged", language = "English",
min_freq = 2, ngrams = TRUE, min_n = 1)
# combine the two lists of keywords
all_keywords <- unique(append(taggedkeywords, rakedkeywords))
# create a co-occurrence network of key terms
dfm <- create_dfm(elements = paste(PMimport$title, PMimport$abstract),
features = all_keywords)
network <- create_network(search_dfm = as.matrix(dfm),
min_studies = 2, min_occ = 2)
# identifying terms based on their importance
cutoff <- find_cutoff(network, method = "cumulative",
percent = .80, imp_method = "strength")
reduced <- reduce_graph(network, cutoff_strength = cutoff[1])
# get those new key words
searchterms <- get_keywords(reduced)
# generate a new PDF for your plot
# this file will save into your working directory
pdf("network.pdf", width = 12, height = 12)
# create a generic network plot
plot(reduced)
# turn off graphics device
dev.off()
# HINT: check the files tab in RStudio for your figure saved as PDF.
# print the new list to the screen
searchterms
library(readxl)
bird_figure <- read_excel("F:/University 2020-2021/bird figure.xlsx")
View(bird_figure)
t.test(weight, wingchord, paired = TRUE, alternative = "two.sided")
View(bird_figure)
knitr::opts_chunk$set(echo = TRUE)
# select new search terms from identified list
# change these from Alicia's terms to your new terms
final <- '(((microbial community) OR (bacterial community) OR (microbiome community)) AND ((community structure) OR (community composition)) AND (core microbiome)) AND (Ecology)'
# get the updated list of PubMedIDs
entrez_F <- get_pubmed_ids(final)
# select new search terms from identified list
# change these from Alicia's terms to your new terms
final <- '(((microbial community) OR (bacterial community) OR (microbiome community)) AND ((community structure) OR (community composition)) AND (core microbiome)) AND (Agricultural)'
# get the updated list of PubMedIDs
entrez_F <- get_pubmed_ids(final)
# select new search terms from identified list
# change these from Alicia's terms to your new terms
final <- '(((microbial community) OR (bacterial community) OR (microbiome community)) AND ((community structure) OR (community composition)) AND (core microbiome)) AND (Medical)'
# get the updated list of PubMedIDs
entrez_F <- get_pubmed_ids(final)
# select new search terms from identified list
# change these from Alicia's terms to your new terms
final <- '(((microbial community) OR (bacterial community) OR (microbiome community)) AND ((community structure) OR (community composition)) AND (core microbiome))'
# get the updated list of PubMedIDs
entrez_F <- get_pubmed_ids(final)
entrez_F$Count
abstracts_F <- batch_pubmed_download(pubmed_query_string = final,
dest_file_prefix = "Microbial_Ecology",
format = "medline",
batch_size = 139)
abstracts_F <- batch_pubmed_download(pubmed_query_string = final,
dest_file_prefix = "Microbial_Ecology",
format = "medline",
batch_size = 139)
Fimport <- import_results(file = "Microbial_Ecology01.txt", verbose = TRUE)
```
# load the file from university 2020-2021, thesis, R, file
Fimport <- import_results(file = "Microbial_Ecology01.txt", verbose = TRUE)
```
# load the file from uni 2020-2021, thesis, R, file
Fimport <- import_results(file = "Microbial_Ecology01.txt", verbose = TRUE)
library(easyPubMed)
library(litsearchr)
Fimport <- import_results(file = "Microbial_Ecology01.txt", verbose = TRUE)
View(Fimport)
Fimport <- import_results(file = "Microbial_Medical01.txt", verbose = TRUE)
View(Fimport)
Fimportmed <- import_results(file = "Microbial_Medical01.txt", verbose = TRUE)
View(Fimport)
Fimporteco <- import_results(file = "Microbial_Ecology01.txt", verbose = TRUE)
View(Fimport)
View(Fimporteco)
View(Fimport)
View(Fimporteco)
View(Fimport)
View(Fimporteco)
knitr::opts_chunk$set(echo = TRUE)
Fimportagri <- import_results(file = "Microbial_Agricultural01.txt", verbose = TRUE)
library(easyPubMed)
library(litsearchr)
Fimportagri <- import_results(file = "Microbial_Agricultural01.txt", verbose = TRUE)
View(Fimport)
View(Fimportmed)
View(Fimporteco)
View(Fimportagri)
View(Fimport)
knitr::opts_chunk$set(echo = TRUE)
# We need to import the PubMed results as a data frame
Riselyimport <- import_results(file = "export.ris", verbose = TRUE)
library(easyPubMed)
library(litsearchr)
# We need to import the PubMed results as a data frame
Riselyimport <- import_results(file = "export.ris", verbose = TRUE)
# Article titles and abstracts often contain key terms.
# Can you identify which columns that information is found in?
rakedkeywords <- extract_terms(text = paste(Riselyimport$title,
Riselyimport$abstract),
method = "fakerake", language = "English",
# How many times should each key term occur (Freq)?
# What is the minimum number of words that need to
# be in each key term (n)?
min_freq = 5, ngrams = TRUE, min_n = 2)
# Did you notice there are some keywords already identified within
# PMimport? These are known as "tagged" keywords. Can you replicate
# the above extract_terms call for the tagged keywords in PMimport?
taggedkeywords <- extract_terms(keywords = PMimport$keywords, method = "tagged", language = "English",
min_freq = 5, ngrams = TRUE, min_n = 1)
# We need to import the PubMed results as a data frame
Riselyimport <- import_results(file = "export.ris", verbose = TRUE)
# Article titles and abstracts often contain key terms.
# Can you identify which columns that information is found in?
rakedkeywords <- extract_terms(text = paste(Riselyimport$title,
Riselyimport$abstract),
method = "fakerake", language = "English",
# How many times should each key term occur (Freq)?
# What is the minimum number of words that need to
# be in each key term (n)?
min_freq = 5, ngrams = TRUE, min_n = 2)
# Did you notice there are some keywords already identified within
# PMimport? These are known as "tagged" keywords. Can you replicate
# the above extract_terms call for the tagged keywords in PMimport?
taggedkeywords <- extract_terms(keywords = Riselyimport$keywords, method = "tagged", language = "English",
min_freq = 5, ngrams = TRUE, min_n = 1)
knitr::opts_chunk$set(echo = TRUE)
library(easyPubMed)
library(litsearchr)
# We need to import the PubMed results as a data frame
Riselyimport <- import_results(file = "export.ris", verbose = TRUE)
# Article titles and abstracts often contain key terms.
# Can you identify which columns that information is found in?
rakedkeywords <- extract_terms(text = paste(Riselyimport$title,
Riselyimport$abstract),
method = "fakerake", language = "English",
# How many times should each key term occur (Freq)?
# What is the minimum number of words that need to
# be in each key term (n)?
min_freq = 5, ngrams = TRUE, min_n = 2)
# Did you notice there are some keywords already identified within
# PMimport? These are known as "tagged" keywords. Can you replicate
# the above extract_terms call for the tagged keywords in PMimport?
taggedkeywords <- extract_terms(keywords = Riselyimport$keywords, method = "tagged", language = "English",
min_freq = 5, ngrams = TRUE, min_n = 1)
View(Riselyimport)
View(rakedkeywords)
# We need to import the PubMed results as a data frame
Riselyimport <- import_results(file = "export.ris", verbose = TRUE)
# Article titles and abstracts often contain key terms.
# Can you identify which columns that information is found in?
rakedkeywords <- extract_terms(text = paste(Riselyimport$title,
Riselyimport$abstract),
method = "fakerake", language = "English",
# How many times should each key term occur (Freq)?
# What is the minimum number of words that need to
# be in each key term (n)?
min_freq = 5, ngrams = TRUE, min_n = 1)
# Did you notice there are some keywords already identified within
# PMimport? These are known as "tagged" keywords. Can you replicate
# the above extract_terms call for the tagged keywords in PMimport?
taggedkeywords <- extract_terms(keywords = Riselyimport$keywords, method = "tagged", language = "English",
min_freq = 5, ngrams = TRUE, min_n = 1)
View(rakedkeywords)
all_KeywordsFiltered <- remove_redundancies(rakedkeywords, closure = "left")
# create a co-occurrence network of key terms
dfm <- create_dfm(elements = paste(Riselyimport$title, Riselyimport$abstract),
features = all_KeywordsFiltered)
network <- create_network(search_dfm = as.matrix(dfm),
min_studies = 2, min_occ = 2)
# identifying terms based on their importance
cutoff <- find_cutoff(network, method = "cumulative",
percent = .80, imp_method = "strength")
reduced <- reduce_graph(network, cutoff_strength = cutoff[1])
# get those new key words
searchterms <- get_keywords(reduced)
View(searchterms)
# We need to import the PubMed results as a data frame
Riselyimport <- import_results(file = "export.ris", verbose = TRUE)
# Article titles and abstracts often contain key terms.
# Can you identify which columns that information is found in?
rakedkeywords <- extract_terms(text = paste(Riselyimport$title,
Riselyimport$abstract),
method = "fakerake", language = "English",
# How many times should each key term occur (Freq)?
# What is the minimum number of words that need to
# be in each key term (n)?
min_freq = 5, ngrams = TRUE, min_n = 2)
all_KeywordsFiltered <- remove_redundancies(rakedkeywords, closure = "left")
# create a co-occurrence network of key terms
dfm <- create_dfm(elements = paste(Riselyimport$title, Riselyimport$abstract),
features = all_KeywordsFiltered)
network <- create_network(search_dfm = as.matrix(dfm),
min_studies = 2, min_occ = 2)
# identifying terms based on their importance
cutoff <- find_cutoff(network, method = "cumulative",
percent = .80, imp_method = "strength")
reduced <- reduce_graph(network, cutoff_strength = cutoff[1])
# get those new key words
searchterms <- get_keywords(reduced)
View(searchterms)
library(easyPubMed)
library(litsearchr)
knitr::opts_chunk$set(echo = TRUE)
library(easyPubMed)
library(litsearchr)
# We need to import the PubMed results as a data frame
Riselyimport <- import_results(file = "export.ris", verbose = TRUE)
# Article titles and abstracts often contain key terms.
# Can you identify which columns that information is found in?
rakedkeywords <- extract_terms(text = paste(Riselyimport$title,
Riselyimport$abstract),
method = "fakerake", language = "English",
# How many times should each key term occur (Freq)?
# What is the minimum number of words that need to
# be in each key term (n)?
min_freq = 5, ngrams = TRUE, min_n = 2)
all_KeywordsFiltered <- remove_redundancies(rakedkeywords, closure = "left")
# create a co-occurrence network of key terms
dfm <- create_dfm(elements = paste(Riselyimport$title, Riselyimport$abstract),
features = all_KeywordsFiltered)
network <- create_network(search_dfm = as.matrix(dfm),
min_studies = 2, min_occ = 2)
# identifying terms based on their importance
cutoff <- find_cutoff(network, method = "cumulative",
percent = .80, imp_method = "strength")
reduced <- reduce_graph(network, cutoff_strength = cutoff[1])
# get those new key words
searchterms <- get_keywords(reduced)
View(Riselyimport)
View(searchterms)
AggregatedData <- '(((microbial community) OR (bacterial community) OR (bacterial communities) OR (microbial communities) OR (microbial species)) AND ((community structure) OR (microbiome project) OR (community composition)) AND (core microbiome))'
# get the updated list of PubMedIDs
CompiledData <- '(((microbial community) OR (bacterial community) OR (bacterial communities) OR (microbial communities) OR (microbial species)) AND ((community structure) OR (microbiome project) OR (community composition)) AND (core microbiome))'
CompiledData <- '(((microbial community) OR (bacterial community) OR (bacterial communities) OR (microbial communities) OR (microbial species)) AND ((community structure) OR (microbiome project) OR (community composition)) AND (core microbiome))'
CompiledData <- '(((microbial community) OR (bacterial community) OR (bacterial communities) OR (microbial communities) OR (microbial species)) AND ((community structure) OR (microbiome project) OR (community composition)) AND (core microbiome))'
# get the updated list of PubMedIDs
entrez_F <- get_pubmed_ids(final)
CompiledData <- '(((microbial community) OR (bacterial community) OR (bacterial communities) OR (microbial communities) OR (microbial species)) AND ((community structure) OR (microbiome project) OR (community composition)) AND (core microbiome))'
# get the updated list of PubMedIDs
entrez_F <- get_pubmed_ids(CompiledData)
View(entrez_F)
entrez_F$Count
abstracts_F <- batch_pubmed_download(pubmed_query_string = CompiledData,
dest_file_prefix = "CompiledData01.txt",
format = "medline",
batch_size = 750)
CompiledData <- '(((microbial community) OR (bacterial community) OR (bacterial communities) OR (microbial communities) OR (microbial species)) AND ((intestinal) OR (gut)) AND ((community structure) OR (microbiome project)) OR (community composition)) AND (core microbiome))'
# get the updated list of PubMedIDs
entrez_F <- get_pubmed_ids(CompiledData)
# How many results did your query return?
entrez_F$Count
CompiledData <- '(((microbial community) OR (bacterial community) OR (bacterial communities) OR (microbial communities) OR (microbial species)) AND ((community structure) OR (microbiome project) OR (community composition)) AND ((core microbiome))'
# get the updated list of PubMedIDs
entrez_F <- get_pubmed_ids(CompiledData)
# How many results did your query return?
entrez_F$Count
abstracts_F <- batch_pubmed_download(pubmed_query_string = CompiledData,
dest_file_prefix = "CompiledData",
format = "medline",
batch_size = 750)
FimportCompiledData <- import_results(file = "CompiledData01.txt", verbose = TRUE)
View(FimportCompiledData)
knitr::opts_chunk$set(echo = TRUE)
CompiledData <- '(((microbial community) OR (bacterial community) OR (bacterial communities) OR (microbial communities) OR (microbial species)) AND ((community structure) OR (microbiome project) OR (community composition)) AND ((core microbiome)) AND (("2018"[Date - Publication] : "2020"[Date - Publication]))'
# get the updated list of PubMedIDs
entrez_F <- get_pubmed_ids(CompiledData)
library(easyPubMed)
library(litsearchr)
CompiledData <- '(((microbial community) OR (bacterial community) OR (bacterial communities) OR (microbial communities) OR (microbial species)) AND ((community structure) OR (microbiome project) OR (community composition)) AND ((core microbiome)) AND (("2018"[Date - Publication] : "2020"[Date - Publication]))'
# get the updated list of PubMedIDs
entrez_F <- get_pubmed_ids(CompiledData)
# How many results did your query return?
entrez_F$Count
abstracts_F <- batch_pubmed_download(pubmed_query_string = CompiledData,
dest_file_prefix = "CompiledData",
format = "medline",
batch_size = 443)
FimportCompiledData <- import_results(file = "CompiledData01.txt", verbose = TRUE)
View(FimportCompiledData)
#random sampling methods
FimportCompiledData[sample(nrow(FimportCompiledData), 3), ]
library(remotes)
remotes::install_github("BirdStudiesCanada/NatureCounts")
library(nature counts)
library(naturecounts)
install.packages("remotes")
install.packages("remotes")
remotes::install_github("BirdStudiesCanada/naturecounts")
library(naturecounts)
library(tidyverse)
setwd("~/")
view(naturecounts_data.txt)
FimportWhips <- import_results(file = "naturecounts_data.txt", verbose = TRUE)
FimportWhips <- import(file = "naturecounts_data.txt", verbose = TRUE)
WPWI <- nc_data_dl(collections = "WPWI", username = "kaelynr", info = "tutorial example")
View(WPWI)
WPWI_filter <- nc_data_dl(collections = "WPWI", years = c(2010, 2012),
region = list(bcr = "13"),
username = "YourUserName", info = "tutorial example")
WPWI_filter <- nc_data_dl(collections = "WPWI", years = c(2010, 2012),
region = list(bcr = "13"),
username = "kaelynr", info = "tutorial example")
WPWI_bb <- nc_data_dl(collections = "WPWI",
region = list(bbox = c(left = -81.7, bottom = 44.5,
right = -80.9, top = 45.3)),
username = "kaelynr", info = "tutorial example")
View(WPWI_bb)
setwd("~/")
knitr::opts_chunk$set(echo = TRUE)
#Identifying papers that are NOT shared by Risely and the subset
import_results(file = "BibChunks/download1.bib", verbose = TRUE) %>%
anti_join(importRisely) %>% select(author, title)
library(car)
knitr::opts_chunk$set(echo = TRUE)
# run every time
# load required packages
library(litsearchr)
#install.packages("revtools")
library(revtools)
library(tidyverse)
# set working directory to file location
setwd("~/University/University 2020-2021/thesis/MicrobiomeThesis")
# read the initial .RIS data into R
importRisely <- import_results(file = "ReferencesRisely.ris", verbose = TRUE)
import <- import_results(file = "ReferencesCompiled01.txt", verbose = TRUE) %>%
# join compiled references from R with Risely papers
full_join(importRisely)
#Identifying papers that are NOT shared by Risely and the subset
import_results(file = "BibChunks/download1.bib", verbose = TRUE) %>%
anti_join(importRisely) %>% select(author, title)
#Identifying papers that are NOT shared by Risely and the subset
import_results(file = "BibChunks/download2.bib", verbose = TRUE) %>%
anti_join(importRisely) %>% select(author, title)
#Identifying papers that are NOT shared by Risely and the subset
import_results(file = "BibChunks/download3.bib", verbose = TRUE) %>%
anti_join(importRisely) %>% select(author, title)
#Identifying papers that are NOT shared by Risely and the subset
import_results(file = "BibChunks/download4.bib", verbose = TRUE) %>%
anti_join(importRisely) %>% select(author, title)
#Identifying papers that are NOT shared by Risely and the subset
import_results(file = "BibChunks/download5.bib", verbose = TRUE) %>%
anti_join(importRisely, by = "title") %>% select(author, title)
#Note that in this subset, Rottjers IS a Risely reference. The accents on this name prevented the code from identifying this.
# AH: added by = "title" to the anti-join, which matches the rows only by identical titles
# AH: this avoids the รถ issue (could do this for all subsets, though not strictly necessary)
#Identifying papers that are NOT shared by Risely and the subset
import_results(file = "BibChunks/download6.bib", verbose = TRUE) %>%
anti_join(importRisely) %>% select(author, title)
library(tidyverse)
library(viridis)
library(googlesheets4)
library(janitor)
library(lubridate)
setwd("~/University/University 2020-2021/thesis/MicrobiomeThesis")
rxk.field <- read_excel("SampleResultsMicrobiome.xlsx")
library(readxl)
SampleResultsMicrobiome <- read_excel("~/University/University 2020-2021/thesis/R/SampleResultsMicrobiome.xlsx")
View(SampleResultsMicrobiome)
# + scale_color/fill_viridis(discrete = T/F)
theme_set(theme_light())
SampleResultsMicrobiome <- gsub("\\.", "", SampleResultsMicrobiome)
fieldWords <- strsplit(SampleResultsMicrobiome, " ")
fieldWordsFreq <- table(unlist(fieldWords))
View(fieldWords)
fieldWords[[11]]
#this gives me all the terms used in the fieldOfStudy column
#but I cannot figure out how to get the frequency of each word, especially because this function picks up "\" and "," as terms
#count function?
count(fieldwords[[11]], "Ecology")
#this gives me all the terms used in the fieldOfStudy column
#but I cannot figure out how to get the frequency of each word, especially because this function picks up "\" and "," as terms
#count function?
count(data = fieldwords[[11]], "Ecology")
#this gives me all the terms used in the fieldOfStudy column
#but I cannot figure out how to get the frequency of each word, especially because this function picks up "\" and "," as terms
#count function?
table(unlist(strsplit(tolower(SampleResultsMicrobiome$fieldOfStudy), " ")))
fieldWords[[11]]
#this gives me all the terms used in the fieldOfStudy column
#but I cannot figure out how to get the frequency of each word, especially because this function picks up "\" and "," as terms
#count function?
#this would allow me to create a table manually
freq.field <- data.frame(fieldOfStudy = c("Ecology", "Medical", "Agricultural", "Industrial"), score = c(23,))
#this gives me all the terms used in the fieldOfStudy column
#but I cannot figure out how to get the frequency of each word, especially because this function picks up "\" and "," as terms
#count function?
#this would allow me to create a table manually
freq.field <- data.frame(fieldOfStudy = c("Ecology", "Medical", "Agricultural", "Industrial"), score = c(23,10,7,3))
View(fieldWords)
View(freq.field)
View(freq.field)
chisq.field <-chisq.test(freq.field)
Chisq.field <- chisq.test(fieldWords[[11]])
Chisq.field <- chisq.test(SampleResultsMicrobiome$fieldOfStdy)
Chisq.field <- chisq.test(fieldOfStdy)
Chisq.field <- chisq.test(fieldOfStudy)
chisq.test(freq.field)
View(freq.field)
#Automating
table(SampleResultsMicrobiome$fieldOfStudy)
View(SampleResultsMicrobiome)
SampleResultsMicrobiome <- read_excel("~/University/University 2020-2021/thesis/R/SampleResultsMicrobiome.xlsx")
View(SampleResultsMicrobiome)
#Automating
table(SampleResultsMicrobiome$fieldOfStudy)
SampleResultsMicrobiome <- read_excel("~/University/University 2020-2021/thesis/R/SampleResultsMicrobiome.xlsx")
