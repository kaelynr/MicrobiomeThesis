all_KeywordsFiltered <- remove_redundancies(rakedkeywords, closure = "left")
# create a co-occurrence network of key terms
dfm <- create_dfm(elements = paste(Riselyimport$title, Riselyimport$abstract),
features = all_KeywordsFiltered)
network <- create_network(search_dfm = as.matrix(dfm),
min_studies = 2, min_occ = 2)
# identifying terms based on their importance
cutoff <- find_cutoff(network, method = "cumulative",
percent = .80, imp_method = "strength")
reduced <- reduce_graph(network, cutoff_strength = cutoff[1])
# get those new key words
searchterms <- get_keywords(reduced)
View(Riselyimport)
View(searchterms)
AggregatedData <- '(((microbial community) OR (bacterial community) OR (bacterial communities) OR (microbial communities) OR (microbial species)) AND ((community structure) OR (microbiome project) OR (community composition)) AND (core microbiome))'
# get the updated list of PubMedIDs
CompiledData <- '(((microbial community) OR (bacterial community) OR (bacterial communities) OR (microbial communities) OR (microbial species)) AND ((community structure) OR (microbiome project) OR (community composition)) AND (core microbiome))'
CompiledData <- '(((microbial community) OR (bacterial community) OR (bacterial communities) OR (microbial communities) OR (microbial species)) AND ((community structure) OR (microbiome project) OR (community composition)) AND (core microbiome))'
CompiledData <- '(((microbial community) OR (bacterial community) OR (bacterial communities) OR (microbial communities) OR (microbial species)) AND ((community structure) OR (microbiome project) OR (community composition)) AND (core microbiome))'
# get the updated list of PubMedIDs
entrez_F <- get_pubmed_ids(final)
CompiledData <- '(((microbial community) OR (bacterial community) OR (bacterial communities) OR (microbial communities) OR (microbial species)) AND ((community structure) OR (microbiome project) OR (community composition)) AND (core microbiome))'
# get the updated list of PubMedIDs
entrez_F <- get_pubmed_ids(CompiledData)
View(entrez_F)
entrez_F$Count
abstracts_F <- batch_pubmed_download(pubmed_query_string = CompiledData,
dest_file_prefix = "CompiledData01.txt",
format = "medline",
batch_size = 750)
CompiledData <- '(((microbial community) OR (bacterial community) OR (bacterial communities) OR (microbial communities) OR (microbial species)) AND ((intestinal) OR (gut)) AND ((community structure) OR (microbiome project)) OR (community composition)) AND (core microbiome))'
# get the updated list of PubMedIDs
entrez_F <- get_pubmed_ids(CompiledData)
# How many results did your query return?
entrez_F$Count
CompiledData <- '(((microbial community) OR (bacterial community) OR (bacterial communities) OR (microbial communities) OR (microbial species)) AND ((community structure) OR (microbiome project) OR (community composition)) AND ((core microbiome))'
# get the updated list of PubMedIDs
entrez_F <- get_pubmed_ids(CompiledData)
# How many results did your query return?
entrez_F$Count
abstracts_F <- batch_pubmed_download(pubmed_query_string = CompiledData,
dest_file_prefix = "CompiledData",
format = "medline",
batch_size = 750)
FimportCompiledData <- import_results(file = "CompiledData01.txt", verbose = TRUE)
View(FimportCompiledData)
knitr::opts_chunk$set(echo = TRUE)
CompiledData <- '(((microbial community) OR (bacterial community) OR (bacterial communities) OR (microbial communities) OR (microbial species)) AND ((community structure) OR (microbiome project) OR (community composition)) AND ((core microbiome)) AND (("2018"[Date - Publication] : "2020"[Date - Publication]))'
# get the updated list of PubMedIDs
entrez_F <- get_pubmed_ids(CompiledData)
library(easyPubMed)
library(litsearchr)
CompiledData <- '(((microbial community) OR (bacterial community) OR (bacterial communities) OR (microbial communities) OR (microbial species)) AND ((community structure) OR (microbiome project) OR (community composition)) AND ((core microbiome)) AND (("2018"[Date - Publication] : "2020"[Date - Publication]))'
# get the updated list of PubMedIDs
entrez_F <- get_pubmed_ids(CompiledData)
# How many results did your query return?
entrez_F$Count
abstracts_F <- batch_pubmed_download(pubmed_query_string = CompiledData,
dest_file_prefix = "CompiledData",
format = "medline",
batch_size = 443)
FimportCompiledData <- import_results(file = "CompiledData01.txt", verbose = TRUE)
View(FimportCompiledData)
#random sampling methods
FimportCompiledData[sample(nrow(FimportCompiledData), 3), ]
library(remotes)
remotes::install_github("BirdStudiesCanada/NatureCounts")
library(nature counts)
library(naturecounts)
install.packages("remotes")
install.packages("remotes")
remotes::install_github("BirdStudiesCanada/naturecounts")
library(naturecounts)
library(tidyverse)
setwd("~/")
view(naturecounts_data.txt)
FimportWhips <- import_results(file = "naturecounts_data.txt", verbose = TRUE)
FimportWhips <- import(file = "naturecounts_data.txt", verbose = TRUE)
WPWI <- nc_data_dl(collections = "WPWI", username = "kaelynr", info = "tutorial example")
View(WPWI)
WPWI_filter <- nc_data_dl(collections = "WPWI", years = c(2010, 2012),
region = list(bcr = "13"),
username = "YourUserName", info = "tutorial example")
WPWI_filter <- nc_data_dl(collections = "WPWI", years = c(2010, 2012),
region = list(bcr = "13"),
username = "kaelynr", info = "tutorial example")
WPWI_bb <- nc_data_dl(collections = "WPWI",
region = list(bbox = c(left = -81.7, bottom = 44.5,
right = -80.9, top = 45.3)),
username = "kaelynr", info = "tutorial example")
View(WPWI_bb)
setwd("~/")
knitr::opts_chunk$set(echo = TRUE)
```{r subset1a}
# not re-run when creating next subset
# randomly select articles (can change number)
set <- import[sample(nrow(import), 5),]
importSub <- import_results(file = "BibChunks/download1.bib", verbose = TRUE) %>%
# additional files for import go here, like for initial import above
# make sure NA's are NA's not strings (reads bib format funny)
mutate_all(~ replace(., .=='NA', NA)) %>%
# remove the articles already assessed from the original download
# sorting by title only to streamline the anti_join
anti_join(import, ., by = "title")
# run every time
# load required packages
library(litsearchr)
#install.packages("revtools")
library(revtools)
library(tidyverse)
setwd("~/University/University 2020-2021/thesis/MicrobiomeThesis/BibChunks")
# read the initial .RIS data into R
import <- import_results(file = "ReferencesCompiled01.txt", verbose = TRUE) %>%
# join compiled references from R with Risely papers
full_join(import_results(file = "ReferencesRisely.ris", verbose = TRUE))
set <- import[sample(nrow(import), 5),]
# writing file called download1 to BibChunks folder (could use better names)
# writing to bib to ensure all info is captured (even columns with NA)
write_bibliography(set, "BibChunks/download1.bib", format = "bib")
importSub <- import_results(file = "BibChunks/download1.bib", verbose = TRUE) %>%
# additional files for import go here, like for initial import above
# make sure NA's are NA's not strings (reads bib format funny)
mutate_all(~ replace(., .=='NA', NA)) %>%
# remove the articles already assessed from the original download
# sorting by title only to streamline the anti_join
anti_join(import, ., by = "title")
importSub2 <- import_results(file = "BibChunks/download2.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub, ., by = "title") # add previous subset number
# this is re-run for every new subset
# replace #'s with subset number
importSub3 <- import_results (file = "BibChunks/download3.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub2, ., by = "title") # add previous subset number
# this is re-run for every new subset
# replace #'s with subset number
importSub2 <- import_results(file = "BibChunks/download2.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub, ., by = "title") # add previous subset number
# this is re-run for every new subset
# replace #'s with subset number
importSub3 <- import_results (file = "BibChunks/download3.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub2, ., by = "title") # add previous subset number
# this is re-run for every new subset
# replace #'s with subset number
importSub2 <- importSub_results(file = "BibChunks/download2.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub, ., by = "title") # add previous subset number
# this is re-run for every new subset
# replace #'s with subset number
importSub3 <- import_results (file = "BibChunks/download3.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub2, ., by = "title") # add previous subset number
# this is re-run for every new subset
# replace #'s with subset number
importSub2 <- import_results(file = "BibChunks/download2.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub, ., by = "title") # add previous subset number
# this is re-run for every new subset
# replace #'s with subset number
importSub3 <- import_results (file = "BibChunks/download3.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub2, ., by = "title") # add previous subset number
# this is re-run for every new subset
# replace #'s with subset number
importSub3 <- import_results(file = "BibChunks/download3.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub2, ., by = "title") # add previous subset number
importSub3 <- import_results(file = "BibChunks/download3.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub2, ., by = "title") # add previous subset number
setwd("~/University/University 2020-2021/thesis/MicrobiomeThesis")
# this is re-run for every new subset
# replace #'s with subset number
importSub3 <- import_results(file = "BibChunks/download3.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub2, ., by = "title") # add previous subset number
View(importSub2)
setwd("~/University/University 2020-2021/thesis/MicrobiomeThesis/BibChunks")
# this is re-run for every new subset
# replace #'s with subset number
importSub2 <- import_results(file = "BibChunks/download2.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub, ., by = "title") # add previous subset number
# this is re-run for every new subset
# replace #'s with subset number
importSub3 <- import_results(file = "BibChunks/download3.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub2, ., by = "title") # add previous subset number
# this is re-run for every new subset
# replace #'s with subset number
importSub2 <- import_results(file = "BibChunks/download2.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub, ., by = "title") # add previous subset number
setwd("~/University/University 2020-2021/thesis/MicrobiomeThesis/BibChunks")
# run every time
# load required packages
library(litsearchr)
#install.packages("revtools")
library(revtools)
library(tidyverse)
# set working directory to file location
setwd("~/University/University 2020-2021/thesis/MicrobiomeThesis/BibChunks")
# read the initial .RIS data into R
import <- import_results(file = "ReferencesCompiled01.txt", verbose = TRUE) %>%
# join compiled references from R with Risely papers
full_join(import_results(file = "ReferencesRisely.ris", verbose = TRUE))
# not re-run when creating next subset
# randomly select articles (can change number)
set <- import[sample(nrow(import), 5),]
# writing file called download1 to BibChunks folder (could use better names)
# writing to bib to ensure all info is captured (even columns with NA)
write_bibliography(set, "BibChunks/download1.bib", format = "bib")
# now we have a random subset of the articles to work with
# in a bibtex file that can be loaded into Mendeley
# this is re-run for every new subset
# this object should have 5 fewer articles than the previous import
importSub <- import_results(file = "BibChunks/download1.bib", verbose = TRUE) %>%
# additional files for import go here, like for initial import above
# make sure NA's are NA's not strings (reads bib format funny)
mutate_all(~ replace(., .=='NA', NA)) %>%
# remove the articles already assessed from the original download
# sorting by title only to streamline the anti_join
anti_join(import, ., by = "title")
# this is the list of articles the next subset is made from
# this is re-run for every new subset
# replace #'s with subset number
importSub2 <- import_results(file = "BibChunks/download2.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub, ., by = "title") # add previous subset number
# this is re-run for every new subset
# replace #'s with subset number
importSub3 <- import_results(file = "BibChunks/download3.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub2, ., by = "title") # add previous subset number
# not re-run when creating next subset
set <- importSub[sample(nrow(importSub), 5),]
write_bibliography(set, "BibChunks/download2.bib", format = "bib")
# this is re-run for every new subset
# replace #'s with subset number
importSub2 <- import_results(file = "BibChunks/download2.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub, ., by = "title") # add previous subset number
# not re-run when creating next subset
set <- importSub2[sample(nrow(importSub2), 5),]
write_bibliography(set, "BibChunks/download3.bib", format = "bib")
# this is re-run for every new subset
# replace #'s with subset number
importSub3 <- import_results(file = "BibChunks/download3.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub2, ., by = "title") # add previous subset number
# not re-run when creating next subset
# randomly select articles (can change number)
set <- import[sample(nrow(import), 5),]
# writing file called download1 to BibChunks folder (could use better names)
# writing to bib to ensure all info is captured (even columns with NA)
write_bibliography(set, "BibChunks/download1.bib", format = "bib")
# now we have a random subset of the articles to work with
# in a bibtex file that can be loaded into Mendeley
# this is re-run for every new subset
# this object should have 5 fewer articles than the previous import
importSub <- import_results(file = "BibChunks/download1.bib", verbose = TRUE) %>%
# additional files for import go here, like for initial import above
# make sure NA's are NA's not strings (reads bib format funny)
mutate_all(~ replace(., .=='NA', NA)) %>%
# remove the articles already assessed from the original download
# sorting by title only to streamline the anti_join
anti_join(import, ., by = "title")
# this is the list of articles the next subset is made from
# not re-run when creating next subset
set <- importSub[sample(nrow(importSub), 5),]
write_bibliography(set, "BibChunks/download2.bib", format = "bib")
# this is re-run for every new subset
# replace #'s with subset number
importSub2 <- import_results(file = "BibChunks/download2.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub, ., by = "title") # add previous subset number
# not re-run when creating next subset
set <- importSub2[sample(nrow(importSub2), 5),]
write_bibliography(set, "BibChunks/download3.bib", format = "bib")
# this is re-run for every new subset
# replace #'s with subset number
importSub3 <- import_results(file = "BibChunks/download3.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub2, ., by = "title") # add previous subset number
# not re-run when creating next subset
set <- importSub3[sample(nrow(importSub3), 5),]
write_bibliography(set, "BibChunks/download4.bib", format = "bib")
# this is re-run for every new subset
# replace #'s with subset number
importSub4 <- import_results (file = "BibChunks/download4.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub3, ., by = "title") # add previous subset number
# not re-run when creating next subset
set <- importSub4[sample(nrow(importSub4), 5),]
write_bibliography(set, "BibChunks/download5.bib", format = "bib")
# this is re-run for every new subset
# replace #'s with subset number
importSub5 <- import_results (file = "BibChunks/download5.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub4, ., by = "title") # add previous subset number
# not re-run when creating next subset
set <- importSub5[sample(nrow(importSub5), 5),]
write_bibliography(set, "BibChunks/download6.bib", format = "bib")
# this is re-run for every new subset
# replace #'s with subset number
importSub6 <- import_results (file = "BibChunks/download6.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub5, ., by = "title") # add previous subset number
# not re-run when creating next subset
set <- importSub6[sample(nrow(importSub6), 5),]
write_bibliography(set, "BibChunks/download7.bib", format = "bib")
# this is re-run for every new subset
# replace #'s with subset number
importSub7 <- import_results (file = "BibChunks/download7.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub6, ., by = "title") # add previous subset number
# not re-run when creating next subset
set <- importSub7[sample(nrow(importSub7), 5),]
write_bibliography(set, "BibChunks/download8.bib", format = "bib")
# this is re-run for every new subset
# replace #'s with subset number
importSub8 <- import_results (file = "BibChunks/download8.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub7, ., by = "title") # add previous subset number
# not re-run when creating next subset
set <- importSub8[sample(nrow(importSub8), 5),]
write_bibliography(set, "BibChunks/download9.bib", format = "bib")
# this is re-run for every new subset
# replace #'s with subset number
importSub9 <- import_results (file = "BibChunks/download9.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub8, ., by = "title") # add previous subset number
# not re-run when creating next subset
set <- importSub9[sample(nrow(importSub9), 5),]
write_bibliography(set, "BibChunks/download10.bib", format = "bib")
# this is re-run for every new subset
# replace #'s with subset number
importSub10 <- import_results (file = "BibChunks/download10.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub9, ., by = "title") # add previous subset number
View(download1.bib)
download1.bib <- import_results(file = "download1.bib", verbose = TRUE)
setwd("~/University/University 2020-2021/thesis/MicrobiomeThesis/BibChunks")
download1.bib <- import_results(file = "download1.bib", verbose = TRUE)
View(download1.bib)
download2.bib <- import_results(file = "download2.bib", verbose = TRUE)
setwd("~/University/University 2020-2021/thesis/MicrobiomeThesis/BibChunks")
download2.bib <- import_results(file = "download2.bib", verbose = TRUE)
View(download2.bib)
setwd("~/University/University 2020-2021/thesis/MicrobiomeThesis/BibChunks")
download3.bib <- import_results(file = "download3.bib", verbose = TRUE)
View(download3.bib)
setwd("~/University/University 2020-2021/thesis/MicrobiomeThesis/BibChunks")
download4.bib <- import_results(file = "download4.bib", verbose = TRUE)
View(download4.bib)
setwd("~/University/University 2020-2021/thesis/MicrobiomeThesis/BibChunks")
download5.bib <- import_results(file = "download5.bib", verbose = TRUE)
setwd("~/University/University 2020-2021/thesis/MicrobiomeThesis/BibChunks")
download5.bib <- import_results(file = "download5.bib", verbose = TRUE)
View(download5.bib)
setwd("~/University/University 2020-2021/thesis/MicrobiomeThesis/BibChunks")
download6.bib <- import_results(file = "download6.bib", verbose = TRUE)
View(download6.bib)
setwd("~/University/University 2020-2021/thesis/MicrobiomeThesis/BibChunks")
download7.bib <- import_results(file = "download7.bib", verbose = TRUE)
View(download7.bib)
setwd("~/University/University 2020-2021/thesis/MicrobiomeThesis/BibChunks")
download8.bib <- import_results(file = "download8.bib", verbose = TRUE)
View(download8.bib)
setwd("~/University/University 2020-2021/thesis/MicrobiomeThesis/BibChunks")
download9.bib <- import_results(file = "download9.bib", verbose = TRUE)
View(download9.bib)
setwd("~/University/University 2020-2021/thesis/MicrobiomeThesis/BibChunks")
download10.bib <- import_results(file = "download10.bib", verbose = TRUE)
View(download10.bib)
setwd("~/University/University 2020-2021/thesis/MicrobiomeThesis")
library(litsearchr)
#install.packages("revtools")
library(revtools)
library(tidyverse)
# read the initial .RIS data into R
import <- import_results(file = "ReferencesCompiled01.txt", verbose = TRUE) %>%
# join compiled references from R with Risely papers
full_join(import_results(file = "ReferencesRisely.ris", verbose = TRUE))
# this is re-run for every new subset
# this object should have 5 fewer articles than the previous import
importSub <- import_results(file = "BibChunks/download1.bib", verbose = TRUE) %>%
# additional files for import go here, like for initial import above
# make sure NA's are NA's not strings (reads bib format funny)
mutate_all(~ replace(., .=='NA', NA)) %>%
# remove the articles already assessed from the original download
# sorting by title only to streamline the anti_join
anti_join(import, ., by = "title")
# this is the list of articles the next subset is made from
# this is re-run for every new subset
# replace #'s with subset number
importSub2 <- import_results(file = "BibChunks/download2.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub, ., by = "title") # add previous subset number
# this is re-run for every new subset
# replace #'s with subset number
importSub3 <- import_results(file = "BibChunks/download3.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub2, ., by = "title") # add previous subset number
# this is re-run for every new subset
# replace #'s with subset number
importSub4 <- import_results (file = "BibChunks/download4.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub3, ., by = "title") # add previous subset number
# this is re-run for every new subset
# replace #'s with subset number
importSub5 <- import_results (file = "BibChunks/download5.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub4, ., by = "title") # add previous subset number
# this is re-run for every new subset
# replace #'s with subset number
importSub6 <- import_results (file = "BibChunks/download6.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub5, ., by = "title") # add previous subset number
# this is re-run for every new subset
# replace #'s with subset number
importSub7 <- import_results (file = "BibChunks/download7.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub6, ., by = "title") # add previous subset number
# this is re-run for every new subset
# replace #'s with subset number
importSub8 <- import_results (file = "BibChunks/download8.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub7, ., by = "title") # add previous subset number
# this is re-run for every new subset
# replace #'s with subset number
importSub9 <- import_results (file = "BibChunks/download9.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub8, ., by = "title") # add previous subset number
# this is re-run for every new subset
# replace #'s with subset number
importSub10 <- import_results (file = "BibChunks/download10.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub9, ., by = "title") # add previous subset number
# this is re-run for every new subset
# replace #'s with subset number
importSub11 <- import_results (file = "BibChunks/download11.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub10, ., by = "title") # add previous subset number
# not re-run when creating next subset
set <- importSub10[sample(nrow(importSub10), 5),]
write_bibliography(set, "BibChunks/download11.bib", format = "bib")
# this is re-run for every new subset
# replace #'s with subset number
importSub11 <- import_results (file = "BibChunks/download11.bib", verbose = TRUE) %>%
mutate_all(~ replace(., .=='NA', NA)) %>%
anti_join(importSub10, ., by = "title") # add previous subset number
download1.bib <- import_results(file = "BibChunks/download1.bib", verbose = TRUE)
View(download1.bib)
importRisely <- import_results(file = "ReferencesRisely.ris", verbose = TRUE)
match_df(importSub, importRisely)
plyr::match_df(importSub, importRisely)
install.packages("plyr")
library(plyr)
match_df(importSub, importRisely)
match_df(importRisely,importSub)
anti_join(importRisely,importSub)
anti_join(importRisely,importSub2)
View(importSub)
View(importSub2)
import_results(file = "BibChunks/download1.bib", verbose = TRUE) %>% anti_join(importRisely)
import_results(file = "BibChunks/download1.bib", verbose = TRUE) %>% anti_join(importRisely,.)
import_results(file = "BibChunks/download1.bib", verbose = TRUE) %>% anti_join(importRisely)
#Identifying papers that are NOT shared by Risely and the subset
import_results(file = "BibChunks/download1.bib", verbose = TRUE) %>% anti_join(importRisely) %>%
select(title, author)
#Identifying papers that are NOT shared by Risely and the subset
import_results(file = "BibChunks/download1.bib", verbose = TRUE) %>% anti_join(importRisely) %>%
select(author, title)
#Identifying papers that are NOT shared by Risely and the subset
import_results(file = "BibChunks/download2.bib", verbose = TRUE) %>% anti_join(importRisely) %>%
select(author, title)
#Identifying papers that are NOT shared by Risely and the subset
import_results(file = "BibChunks/download3.bib", verbose = TRUE) %>% anti_join(importRisely) %>%
select(author, title)
#Identifying papers that are NOT shared by Risely and the subset
import_results(file = "BibChunks/download5.bib", verbose = TRUE) %>% anti_join(importRisely) %>%
select(author, title)
#Identifying papers that are NOT shared by Risely and the subset
import_results(file = "BibChunks/download5.bib", verbose = TRUE) %>% anti_join(importRisely) %>%
select(author, title)
#Identifying papers that are NOT shared by Risely and the subset
import_results(file = "BibChunks/download4.bib", verbose = TRUE) %>% anti_join(importRisely) %>%
select(author, title)
#Identifying papers that are NOT shared by Risely and the subset
import_results(file = "BibChunks/download3.bib", verbose = TRUE) %>% anti_join(importRisely) %>%
select(author, title)
#Identifying papers that are NOT shared by Risely and the subset
import_results(file = "BibChunks/download6.bib", verbose = TRUE) %>% anti_join(importRisely) %>%
select(author, title)
#Identifying papers that are NOT shared by Risely and the subset
import_results(file = "BibChunks/download8.bib", verbose = TRUE) %>% anti_join(importRisely) %>%
select(author, title)
#Identifying papers that are NOT shared by Risely and the subset
import_results(file = "BibChunks/download11.bib", verbose = TRUE) %>% anti_join(importRisely) %>%
select(author, title)
#Identifying papers that are NOT shared by Risely and the subset
import_results(file = "BibChunks/download10.bib", verbose = TRUE) %>% anti_join(importRisely) %>%
select(author, title)
#Identifying papers that are NOT shared by Risely and the subset
import_results(file = "BibChunks/download9.bib", verbose = TRUE) %>% anti_join(importRisely) %>%
select(author, title)
#Identifying papers that are NOT shared by Risely and the subset
import_results(file = "BibChunks/download8.bib", verbose = TRUE) %>% anti_join(importRisely) %>%
select(author, title)
